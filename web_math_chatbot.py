import streamlit as st
import google.generativeai as genai
import os
import io # BytesIO ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
import PIL.Image # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°

# --- ëª¨ë¸ ì„¤ì • ---
AVAILABLE_MODELS = {
    "âš¡ï¸ Gemini 2.0 Flash (ë¹ ë¦„, íš¨ìœ¨ì )": "gemini-2.0-flash",
    "ğŸš€ Gemini 2.5 Pro Exp (ì‹¤í—˜ìš©, ê³ ì„±ëŠ¥)": "gemini-2.5-pro-exp-03-25",
}
DEFAULT_MODEL_NAME = "âš¡ï¸ Gemini 2.0 Flash (ë¹ ë¦„, íš¨ìœ¨ì )"

# --- ë¹„ë°€ë²ˆí˜¸ í™•ì¸ í•¨ìˆ˜ ---
def check_password():
    # (ë¹„ë°€ë²ˆí˜¸ í™•ì¸ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
    try:
        correct_password = st.secrets.get("APP_PASSWORD", "test1234")
    except Exception:
        correct_password = "test1234"
        if "password_warning_shown" not in st.session_state:
             st.warning("âš ï¸ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì„ì‹œ ë¹„ë°€ë²ˆí˜¸ 'test1234'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°°í¬ ì‹œì—ëŠ” Streamlit Secretsì— 'APP_PASSWORD'ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
             st.session_state.password_warning_shown = True
    password_placeholder = st.empty()
    password = password_placeholder.text_input("ğŸ”‘ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password", key="password_input")
    if not password:
        st.info("ì±—ë´‡ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()
    elif password == correct_password:
        password_placeholder.empty()
        return True
    else:
        st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.stop()
        return False

# --- Gemini API í˜¸ì¶œ í•¨ìˆ˜ ---
def get_gemini_response(prompt_parts, model_display_name, model_object):
    """ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ê°ì²´ë¡œ Gemini APIë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
    gemini_response_text = ""
    try:
        response = model_object.generate_content(prompt_parts, stream=False)
        if hasattr(response, 'text'):
             gemini_response_text = response.text
        elif response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
             gemini_response_text = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))
        else:
             safety_feedback = response.prompt_feedback
             block_reason = safety_feedback.block_reason if hasattr(safety_feedback, 'block_reason') else "ì•Œ ìˆ˜ ì—†ìŒ"
             gemini_response_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ìœ : {block_reason})"
    except Exception as e:
        st.error(f"Gemini API ({model_object.model_name}) í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        gemini_response_text = "ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë‹µë³€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return gemini_response_text

# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì…”í‹€", page_icon="ğŸš€")

# --- í˜ì´ì§€ ì‹œì‘ ì‹œ ë¹„ë°€ë²ˆí˜¸ í™•ì¸ ---
if check_password():
    # --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_image_bytes" not in st.session_state: # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ bytesë¡œ ì €ì¥
        st.session_state.current_image_bytes = None
    if "last_processed_image_id" not in st.session_state: # ë§ˆì§€ë§‰ ìë™ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ID
        st.session_state.last_processed_image_id = None

    # --- ì›¹í˜ì´ì§€ UI ---
    st.title("ğŸ”¢ ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì…”í‹€ ğŸš€")
    st.caption("Gemini AIê°€ ì´ë¯¸ì§€ ì† ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì–´ë“œë¦½ë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("âš™ï¸ ëª¨ë¸ ì„¤ì •")
    selected_model_display_name = st.selectbox(
        "ì‚¬ìš©í•  Gemini ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        options=list(AVAILABLE_MODELS.keys()),
        index=list(AVAILABLE_MODELS.keys()).index(DEFAULT_MODEL_NAME),
        key="selected_model"
    )
    selected_model_id = AVAILABLE_MODELS[selected_model_display_name]
    st.caption(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ID: `{selected_model_id}`")
    st.markdown("---")

    # --- Gemini API ì„¤ì • ---
    API_KEY = st.secrets.get("GEMINI_API_KEY", None)
    if not API_KEY:
        st.error("âš ï¸ ì¤‘ìš”: Streamlit Secretsì— 'GEMINI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    try:
        genai.configure(api_key=API_KEY)
        model = genai.GenerativeModel(selected_model_id)
    except Exception as e:
        st.error(f"ì•—! Gemini ëª¨ë¸({selected_model_id})ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”: {e}")
        st.warning("ì„ íƒí•œ ëª¨ë¸ ì´ë¦„ì´ ì •í™•í•œì§€, API í‚¤ì— í•´ë‹¹ ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    # --- ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬ ---
    uploaded_file = st.file_uploader(
        "ì—¬ê¸°ì— ìˆ˜í•™ ë¬¸ì œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PNG, JPG)",
        type=["png", "jpg", "jpeg"],
        key="file_uploader" # keyë¥¼ ì§€ì •í•˜ì—¬ ìƒíƒœ ì¶”ì  ìš©ì´
    )

    # ìƒˆ ì´ë¯¸ì§€ ì—…ë¡œë“œ ê°ì§€ ë° ìë™ ì²˜ë¦¬
    if uploaded_file is not None:
        # ì„¸ì…˜ì— í˜„ì¬ ì´ë¯¸ì§€ ì €ì¥ (bytes í˜•íƒœë¡œ)
        current_bytes = uploaded_file.getvalue()
        st.session_state.current_image_bytes = current_bytes
        current_image_id = uploaded_file.id # íŒŒì¼ ì—…ë¡œë”ëŠ” ê³ ìœ  ID ì œê³µ

        # í™”ë©´ì— ì´ë¯¸ì§€ í‘œì‹œ (ì´ì „ì— í‘œì‹œëœ ì´ë¯¸ì§€ì™€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ)
        # (ì£¼ì˜: Streamlitì€ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰ ì‹œ ìœ„ì ¯ ìƒíƒœ ìœ ì§€í•˜ë¯€ë¡œ, í•­ìƒ ì´ë¯¸ì§€ í‘œì‹œ í•„ìš”)
        st.image(current_bytes, caption="ì—…ë¡œë“œëœ ë¬¸ì œ ì´ë¯¸ì§€", width=300)

        # ì´ ì´ë¯¸ì§€ê°€ ì´ì „ì— ìë™ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²˜ë¦¬ ì‹œì‘
        if current_image_id != st.session_state.get("last_processed_image_id"):
            st.info(f"ìƒˆ ì´ë¯¸ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. {selected_model_display_name}ì—ê²Œ ìë™ í’€ì´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤...")
            st.session_state.messages = [] # ìƒˆ ì´ë¯¸ì§€ì´ë¯€ë¡œ ì´ì „ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)

            try:
                img = PIL.Image.open(io.BytesIO(current_bytes)) # bytesì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ

                # ìë™ í’€ì´ìš© í”„ë¡¬í”„íŠ¸
                auto_solve_prompt = [
                    f"""ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì˜ ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ìµœëŒ€í•œ ìì„¸í•œ í’€ì´ë¥¼ ì œê³µí•˜ì—¬ì„œ, ì²¨ë¶€í•œ ì´ë¯¸ì§€ ë‚´ì˜ ìˆ˜í•™ë¬¸ì œë¥¼ í’€ì–´ì¤˜.
                    ë§Œì•½ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì²«ë²ˆì§¸ë¡œ ë³´ì´ëŠ” ë¬¸ì œë¥¼ í’€ì–´ì¤˜.
                    ìˆ˜ì‹ì€ LaTeX í˜•ì‹($$...$$ ë˜ëŠ” $$ ... $$)ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                    """,
                    img
                ]

                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    with st.spinner(f"{selected_model_display_name}ê°€ ìë™ í’€ì´ ì¤‘... ğŸ¤”"):
                        # API í˜¸ì¶œ í•¨ìˆ˜ ì‚¬ìš©
                        gemini_response_text = get_gemini_response(
                            auto_solve_prompt,
                            selected_model_display_name,
                            model # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ê°ì²´ ì „ë‹¬
                        )
                    message_placeholder.markdown(gemini_response_text)

                # ìë™ í’€ì´ ê²°ê³¼ ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": gemini_response_text})
                # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ID ì €ì¥ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
                st.session_state.last_processed_image_id = current_image_id

            except Exception as e:
                st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ë˜ëŠ” ìë™ í’€ì´ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ì˜¤ë¥˜ ë©”ì‹œì§€ë„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€ ê°€ëŠ¥
                st.session_state.messages.append({"role": "assistant", "content": f"ì˜¤ë¥˜ ë°œìƒ: {e}"})
                st.session_state.last_processed_image_id = current_image_id # ì˜¤ë¥˜ê°€ ë‚˜ë„ ì¼ë‹¨ ì²˜ë¦¬ëœ ê±¸ë¡œ ê°„ì£¼

    # --- ì±„íŒ… ê¸°ë¡ ì¶œë ¥ ---
    # (ìë™ í’€ì´ ê²°ê³¼ë„ ì—¬ê¸°ì— í¬í•¨ë˜ì–´ ì¶œë ¥ë¨)
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- ì±„íŒ… ì…ë ¥ ì²˜ë¦¬ (ì¶”ê°€ ì§ˆë¬¸ ë˜ëŠ” í…ìŠ¤íŠ¸ ì§ˆë¬¸) ---
    if user_input := st.chat_input(f"{selected_model_display_name}ì—ê²Œ ì§ˆë¬¸í•˜ê¸°..."):
        # ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        prompt_parts = []
        gemini_response_text = ""

        # í˜„ì¬ ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì¶”ê°€ ì§ˆë¬¸ìš©)
        if st.session_state.current_image_bytes is not None:
            st.info(f"ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ëŒ€í•´ {selected_model_display_name}ì—ê²Œ ì¶”ê°€ ì§ˆë¬¸í•©ë‹ˆë‹¤...")
            try:
                img = PIL.Image.open(io.BytesIO(st.session_state.current_image_bytes))
                prompt_parts = [
                    f"""ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì˜ ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ì´ì „ì— ì œì‹œëœ ì´ë¯¸ì§€ì™€ í’€ì´ì— ëŒ€í•´ ë‹¤ìŒ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.
                    ìˆ˜ì‹ì€ LaTeX í˜•ì‹($$...$$ ë˜ëŠ” $$ ... $$)ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

                    ì¶”ê°€ ì§ˆë¬¸: {user_input}
                    """,
                    img
                ]
            except Exception as e:
                 st.error(f"ì´ë¯¸ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                 gemini_response_text = "ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # ì´ë¯¸ì§€ê°€ ì—†ë‹¤ë©´ í…ìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        else:
            st.info(f"í…ìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ {selected_model_display_name}ì—ê²Œ ì§ˆë¬¸í•©ë‹ˆë‹¤...")
            prompt_parts = [
                 f"""ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì˜ ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                 ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ìˆ˜í•™ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ê´€ë ¨ ì—†ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
                 ìˆ˜ì‹ì€ LaTeX í˜•ì‹($$...$$ ë˜ëŠ” $$ ... $$)ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

                 ì§ˆë¬¸: {user_input}
                 """
            ]

        # API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
        if not gemini_response_text and prompt_parts:
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                with st.spinner(f"{selected_model_display_name}ê°€ ë‹µë³€ ì¤€ë¹„ ì¤‘... ğŸ¤”"):
                    # API í˜¸ì¶œ í•¨ìˆ˜ ì‚¬ìš©
                    gemini_response_text = get_gemini_response(
                        prompt_parts,
                        selected_model_display_name,
                        model
                    )
                message_placeholder.markdown(gemini_response_text)

        # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
        if gemini_response_text:
             st.session_state.messages.append({"role": "assistant", "content": gemini_response_text})